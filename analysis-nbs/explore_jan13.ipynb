{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Explore: January 13, 2019_\n",
    "\n",
    "**Objective**: To be determined; however, the following articles/links might provide some inspiration!\n",
    "- [Generating A Twitter Ego-Network & Detecting Communities](https://towardsdatascience.com/generating-twitter-ego-networks-detecting-ego-communities-93897883d255)\n",
    "- [Deploy your side-projects at scale for basically nothing - Google Cloud Run](https://alexolivier.me/posts/deploy-container-stateless-cheap-google-cloud-run-serverless)\n",
    "- [Awesome Streamlit](http://awesome-streamlit.org/)\n",
    "\n",
    "Also, thinking of modeling text of Verified Users, getting more data, and potentially starting development of an app that analyzes RT data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Load in Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "verified = pd.read_json(\"json-data/verified_train.json\", orient=\"split\", dtype={\"id_str\": str})\n",
    "ira = pd.read_json(\"json-data/ira_train.json\", orient=\"split\", dtype={\"id_str\": str}).sample(n=len(verified), random_state=5)\n",
    "\n",
    "# combine dfs above \n",
    "df = pd.concat([verified, ira])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode label column\n",
    "df = pd.get_dummies(df, columns=[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab subset of data to experiment on\n",
    "sample = (df[[\"id_str\", \"screen_name\", \"created_at\", \"full_text\", \"label_real\"]]\n",
    "          .sample(frac=0.50, random_state=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "sample.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Beyond n-grams: word embeddings_\n",
    "\n",
    "- mapping words into an n-dimensional vector space\n",
    "- produced using deep learning and huge amounts of data\n",
    "- discern how similar two words are to each other\n",
    "- used to detect synonyms and antonyms\n",
    "- captures complex relationships\n",
    "- dependent on spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# load model and create Doc object\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"I am happy.\")\n",
    "\n",
    "# generate word vectors for each token\n",
    "#for token in doc:\n",
    "#    print(token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy happy 1.0\n",
      "happy joyous 0.60518235\n",
      "happy sad 0.64389884\n",
      "joyous happy 0.60518235\n",
      "joyous joyous 1.0\n",
      "joyous sad 0.4639511\n",
      "sad happy 0.64389884\n",
      "sad joyous 0.4639511\n",
      "sad sad 1.0\n"
     ]
    }
   ],
   "source": [
    "# word similarities\n",
    "doc = nlp(\"happy joyous sad\")\n",
    "\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document similarities\n",
    "sent1 = nlp(\"I am happy\")\n",
    "sent2 = nlp(\"I am sad\")\n",
    "sent3 = nlp(\"I am joyous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9492464724721577"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute similarity between sent1 and sent2\n",
    "sent1.similarity(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9383192352389133"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute similarity between sent1 and sent3\n",
    "sent1.similarity(sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
